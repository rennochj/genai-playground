

# Core Requirements

* Create a main.py file.
* In that file create a function that will integrate with a Llama4 llm using hosted on ollama.
* You should use langchain as the core framework for integrating with this model.
* The model name is 'llama4' and the model url is 'http://mercury.local:11434'
<!-- * The created function should support the use of tools using model context protocols (MCP) -->





