import json
import logging
from typing import Any, Dict, List, Literal, Optional

import requests
from pydantic import BaseModel, Field

# Configure logger
logger = logging.getLogger(__name__)

"""
Chat Completion Module

This module provides functionality for interacting with AI language models through
a chat completion API. It defines data models, helper functions, and request handling
mechanisms for:

1. Structured chat messages and responses (using Pydantic models)
2. Tool/function calling capabilities (function definitions and execution)
3. Error handling for API interactions
4. Processing model responses and handling chat workflows

The implementation supports modern AI chat capabilities including tool use,
where the model can request external data or functionality through defined tools.
"""

# --------------------------------------------------------------------------------


class ChatMessage(BaseModel):
    """Model for chat messages following OpenAI's chat completion API specification."""

    role: Literal["system", "user", "assistant", "tool", "function"] = Field(
        ..., description="The role of the message author"
    )
    content: Optional[str] = Field(None, description="The content of the message")
    name: Optional[str] = Field(None, description="The name of the author of this message")
    tool_calls: Optional[List[Dict[str, Any]]] = Field(
        None, description="The tool calls generated by the model"
    )
    tool_call_id: Optional[str] = Field(
        None, description="Tool call ID if this is a response from a tool"
    )
    function_call: Optional[Dict[str, Any]] = Field(
        None, description="The function call generated by the model"
    )


# --------------------------------------------------------------------------------


class ChatChoice(BaseModel):
    """Model for a single chat completion choice returned by the API."""

    index: int = Field(..., description="Index of this choice in the list of choices")
    message: ChatMessage = Field(..., description="The chat message generated by the model")
    finish_reason: Optional[
        Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    ] = Field(
        None,
        description="Reason why the model stopped generating: 'stop'=complete, 'length'=max tokens, 'function_call'=function called, etc.",
    )
    logprobs: Optional[Any] = Field(None, description="Log probabilities of output tokens")


# --------------------------------------------------------------------------------


class UsageInfo(BaseModel):
    """Model for token usage statistics from the API response."""

    prompt_tokens: int = Field(..., description="Number of tokens in the prompt")
    completion_tokens: int = Field(..., description="Number of tokens in the completion")
    total_tokens: int = Field(..., description="Total number of tokens used in the request")


# --------------------------------------------------------------------------------


class ModelContextProtocol(BaseModel):
    """Protocol for model context information."""

    model_name: str = Field(..., description="Name of the model being used")
    max_tokens: Optional[int] = Field(None, description="Maximum tokens allowed for completion")
    temperature: float = Field(0.7, description="Sampling temperature for responses")
    top_p: float = Field(1.0, description="Nucleus sampling parameter")
    frequency_penalty: float = Field(0.0, description="Penalty for frequent token usage")
    presence_penalty: float = Field(0.0, description="Penalty for new token usage")
    stop_sequences: Optional[List[str]] = Field(None, description="Sequences that stop generation")
    context_window: Optional[int] = Field(None, description="Maximum context window size")


# --------------------------------------------------------------------------------


class ChatResponse(BaseModel):
    """Model for the complete chat completion API response."""

    id: str = Field(..., description="Unique identifier for this completion")
    object: str = Field("chat.completion", description="Object type, always 'chat.completion'")
    created: int = Field(..., description="Unix timestamp of when the response was created")
    model: str = Field(..., description="Model used for completion")
    system_fingerprint: Optional[str] = Field(
        None, description="Unique identifier for the model version"
    )
    choices: List[ChatChoice] = Field(..., description="List of generated completions")
    usage: UsageInfo = Field(..., description="Statistics about token usage")


# --------------------------------------------------------------------------------


class ChatCompletionError(Exception):
    """
    Custom exception for chat completion errors.

    This exception is raised when there are issues with the chat completion request
    or response processing, such as network errors, invalid responses, or API
    failures.
    """

    pass


# --------------------------------------------------------------------------------


def get_weather(location: str, unit: str = "celsius") -> str:
    """
    Get the current weather for a given location.

    This is a placeholder function that simulates retrieving weather information.
    In a production environment, this would connect to a real weather API service.

    Args:
        location: The name of the location (city, region, etc.) to get weather for
        unit: The temperature unit to use, either "celsius" or "fahrenheit"

    Returns:
        A formatted string containing the weather information
    """
    # Placeholder function to simulate weather retrieval
    # In a real application, this would call a weather API
    return f"The current weather in {location} is sunny with a temperature of 25 degrees {unit}."


# --------------------------------------------------------------------------------


def execute_tool_call(tool_call: Dict[str, Any]) -> Any:
    """
    Execute a tool call and return the result.

    This function parses the tool call request from the model, dispatches it to the
    appropriate handler function based on the function name, and returns the result.

    Args:
        tool_call: A dictionary containing the tool call information, including
                  the function name and arguments as a JSON string

    Returns:
        The result from the executed tool function

    Raises:
        ValueError: If the requested tool/function is not implemented
        Exception: Any exceptions raised by the tool function itself
    """
    function_name = tool_call["function"]["name"]
    arguments = json.loads(tool_call["function"]["arguments"])

    if function_name == "get_weather":
        return get_weather(**arguments)
    else:
        raise ValueError(f"Unknown tool: {function_name}")


# --------------------------------------------------------------------------------


def request(
    model_url: str,
    model_name: str,
    messages: list[ChatMessage],
    tools: Optional[list[Dict[str, Any]]] = None,
    tool_choice: Optional[str] = "auto",
    tool_response_pending: bool = False,
    use_mcp: bool = False,
    **kwargs: Any,
) -> ChatResponse:
    """
    Make a chat completion request to the model endpoint with tool support.

    This function handles the complete chat workflow including:
    1. Formatting and sending the initial request to the model
    2. Processing the model's response
    3. Detecting and executing any tool calls requested by the model
    4. Making follow-up requests with tool results when necessary

    The function supports recursive operation to handle the back-and-forth required
    when the model makes tool calls.

    Args:
        model_url: The URL of the model API endpoint
        model_name: The name of the model to use for completion
        messages: A list of ChatMessage objects representing the conversation history
        tools: Optional list of tool definitions the model can use
        tool_choice: Controls how the model uses tools ("auto", "none", or specific tool)
        tool_response_pending: Flag indicating if we're processing tool execution results
        use_mcp: Use the Model Context Protocol client instead of direct API calls
        **kwargs: Additional parameters to pass to the API

    Returns:
        A ChatResponse object containing the model's final response

    Raises:
        ChatCompletionError: If the API request fails or returns invalid data
    """
    # If MCP client is requested, use it instead of the direct API approach
    if use_mcp:
        try:
            # Import here to avoid circular imports
            import asyncio

            from archive.mcp_client import create_mcp_client

            # Create an MCP client and run the request
            mcp_client = create_mcp_client(model_url, model_name)
            return asyncio.run(mcp_client.request(messages, tools))
        except ImportError:
            logger.warning("mcp_client_import_failed - Using fallback API approach")
        except Exception as e:
            logger.error("mcp_client_request_failed - error: %s", str(e))
            raise ChatCompletionError(f"Failed to make MCP client request: {str(e)}") from e

    # Continue with the existing implementation if not using MCP
    url = model_url

    payload = {
        "model": model_name,
        "messages": [
            {
                "role": msg.role,
                "content": msg.content,
                "tool_calls": msg.tool_calls,
                "tool_call_id": msg.tool_call_id,
            }
            for msg in messages
        ],
        "stream": False,
        "options": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 500},
    }

    if tools:
        payload["tools"] = tools
        payload["tool_choice"] = tool_choice or "auto"

    try:
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers)

        response_data = response.json()
        message = response_data.get("choices", [{}])[0].get("message", {})

        # If we have tool calls and haven't executed them yet
        if "tool_calls" in message and not tool_response_pending:
            tool_calls = message["tool_calls"]

            # Execute tool calls and collect results
            for tool_call in tool_calls:
                try:
                    result = execute_tool_call(tool_call)
                    messages.append(
                        ChatMessage(
                            role="tool",
                            content=str(result),
                            tool_call_id=tool_call["id"],
                            name=None,
                            tool_calls=None,
                            function_call=None,
                        )
                    )
                except Exception as e:
                    messages.append(
                        ChatMessage(
                            role="tool",
                            content=f"Error: {str(e)}",
                            tool_call_id=tool_call["id"],
                            name=None,
                            tool_calls=None,
                            function_call=None,
                        )
                    )

            # Make final request without tools to get natural language response
            return request(
                model_url=model_url,
                model_name=model_name,
                messages=messages,
                tools=None,  # Remove tools to prevent further tool calls
                tool_response_pending=True,
            )

        return ChatResponse(**response_data)

    except requests.RequestException as e:
        logger.error(
            "chat_request_failed - error: %s, url: %s, response_content: %s",
            str(e),
            url,
            getattr(e.response, "text", None),
        )
        raise ChatCompletionError(f"Failed to make chat completion request: {str(e)}") from e
